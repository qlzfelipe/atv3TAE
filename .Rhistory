dtgrid <- data.frame(X1 = x1_grid, X2 = x2_grid)
mdgrid <-  knn(train = train, test = dtgrid, k = selected_k, cl = train$Y)
dtgrid <- data.frame(X1 = x1_grid, X2 = x2_grid, Y = 0)
mdgrid <-  knn(train = train, test = dtgrid, k = selected_k, cl = train$Y)
mdgrid
install.packages("kknn")
library(kknn)
?kknn::kknn
library(nnet)
library(class)
library(kernlab)
library(kknn)
source("utils.R")
link <- "https://raw.githubusercontent.com/andersonara/datasets/master/wall-robot-navigation.csv"
data <- read.csv(url(link), sep = ";")
data$Y <- factor(data$Y, ordered = FALSE)
#Separação - Treinamento e teste
size_tr <- round(.75 * nrow(data))
train_indx <- sample(1:nrow(data), size_tr, replace = FALSE)
train <- data[train_indx, ]
test <- data[-train_indx, ]
#Realização de validação holdout repetido
n_holdout <- 100
hold_prop <- .2
test_holdout_size <- round(hold_prop*nrow(train))
#K_grid
k_grid <- 1:20
mcc_vec <- numeric(length(k_grid))
for(i in 1:length(k_grid)){
mcc_avg <- numeric(n_holdout)
sapply(X = 1:n_holdout, function(x){
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, ]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[j] <<- mcc_custom(holdout_test$Y, model)
print(x)
})
mcc_vec[i] <- mean(mcc_avg)
}
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, ]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
model
model
model$response
fitted(model)
length(holdout_test)
nrow(holdout_test)
holdout_test <- train[test_indx_holdout, -Y]
holdout_test <- train[test_indx_holdout, -"Y"]
holdout_test <- train[test_indx_holdout, -c("Y")]
holdout_test <- train[test_indx_holdout, -c(Y)]
holdout_test <- train[test_indx_holdout, -3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[j] <<- mcc_custom(holdout_test$Y, fitted(model))
fitted(model)
holdout_test$Y
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[j] <<- mcc_custom(holdout_test$Y, fitted(model))
mcc_avg[j] <<- mcc_custom(holdout_test_labels, fitted(model))
#K_grid
k_grid <- 1:20
mcc_vec <- numeric(length(k_grid))
for(i in 1:length(k_grid)){
mcc_avg <- numeric(n_holdout)
sapply(X = 1:n_holdout, function(x){
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[j] <<- mcc_custom(holdout_test_labels, fitted(model))
print(x)
})
mcc_vec[i] <- mean(mcc_avg)
}
mcc_vec
prd <- predict(model, holdout_test)
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
prd <- predict(model, holdout_test)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
prd <- predict(model, holdout_test)
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
prd <- predict(model, holdout_test)
model <- kknn(Y~.,train = holdout_train,
kernel = "rectangular", k = k_grid[i])
model <- kknn(Y~., holdout_train,
kernel = "rectangular", k = k_grid[i])
model <- train.kknn(Y~., holdout_train,
kernel = "rectangular", k = k_grid[i])
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test
View(holdout_test)
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
model
prd <- predict(model, holdout_test)
mcc_avg
holdout_test_labels
fitted(model)
holdout_test_labels
mcc_avg[j] <<- mcc_custom(holdout_test_labels, fitted(model))
mcc_avg
library(nnet)
library(class)
library(kernlab)
library(kknn)
source("utils.R")
link <- "https://raw.githubusercontent.com/andersonara/datasets/master/wall-robot-navigation.csv"
data <- read.csv(url(link), sep = ";")
data$Y <- factor(data$Y, ordered = FALSE)
#Separação - Treinamento e teste
size_tr <- round(.75 * nrow(data))
train_indx <- sample(1:nrow(data), size_tr, replace = FALSE)
train <- data[train_indx, ]
test <- data[-train_indx, ]
#Realização de validação holdout repetido
n_holdout <- 100
hold_prop <- .2
test_holdout_size <- round(hold_prop*nrow(train))
#K_grid
k_grid <- 1:20
mcc_vec <- numeric(length(k_grid))
for(i in 1:length(k_grid)){
mcc_avg <- numeric(n_holdout)
sapply(X = 1:n_holdout, function(x){
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[j] <<- mcc_custom(holdout_test_labels, fitted(model))
print(x)
})
mcc_vec[i] <- mean(mcc_avg)
}
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
model
length(fitted(model))
mcc_avg
library(nnet)
library(class)
library(kernlab)
library(kknn)
source("utils.R")
link <- "https://raw.githubusercontent.com/andersonara/datasets/master/wall-robot-navigation.csv"
data <- read.csv(url(link), sep = ";")
data$Y <- factor(data$Y, ordered = FALSE)
#Separação - Treinamento e teste
size_tr <- round(.75 * nrow(data))
train_indx <- sample(1:nrow(data), size_tr, replace = FALSE)
train <- data[train_indx, ]
test <- data[-train_indx, ]
#Realização de validação holdout repetido
n_holdout <- 100
hold_prop <- .2
test_holdout_size <- round(hold_prop*nrow(train))
#K_grid
k_grid <- 1:20
mcc_vec <- numeric(length(k_grid))
for(i in 1:length(k_grid)){
mcc_avg <- numeric(n_holdout)
sapply(X = 1:n_holdout, function(x){
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[x] <<- mcc_custom(holdout_test_labels, fitted(model))
print(x)
})
mcc_vec[i] <- mean(mcc_avg)
}
selected_k <- k_grid[which.min(mcc_vec)]
selected_k
knn_model_fit <- kknn(Y~.,train = train, test = train, k = selected_k,
kernel = "rectangular")
knn_model_final <- kknn(Y~.,train = train, test = test, k = selected_k,
kernel = "rectangular")
mcc_custom(test$Y, knn_model_final)
mcc_custom(test$Y, fitted(knn_model_final))
mcc_custom(train$Y, fitted(knn_model_fit))
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 1000)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 1000)
dtgrid <- data.frame(X1 = x1_grid, X2 = x2_grid, Y = 0)
mdgrid <-  kknn(train = train, test = dtgrid, k = selected_k)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 1000)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 1000)
dtgrid <- data.frame(X1 = x1_grid, X2 = x2_grid)
mdgrid <-  kknn(train = train, test = dtgrid, k = selected_k)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
dtgrid <- cbind(dtgrid, fitted(mdgrid))
View(dtgrid)
unique(dtgrid$`fitted(mdgrid)`)
selected_k
unique(data$Y)
table(data$Y)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 10000)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 10000)
dtgrid <- data.frame(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
dtgrid <- cbind(dtgrid, fitted(mdgrid))
unique(dtgrid$`fitted(mdgrid)`)
min(train$X2)
max(train$X2)
View(data)
View(dtgrid)
summary(data)
plot(data$X1, data$X2)
plot(data$X1, data$X2, col = data$Y)
colnames(dtgrid)[3] <- "Y"
plot(dtgrid$X1, dtgrid$X2, col = dtgrid$Y)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 1000)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 1000)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 100)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 100)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
dtgrid <- rbind(dtgrid, fitted(mdgrid))
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
dtgrid <- cbind(dtgrid, fitted(mdgrid))
colnames(dtgrid)[3] <- "Y"
plot(dtgrid$X1, dtgrid$X2, col = dtgrid$Y)
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y)
?points
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, alpha = 1)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = 1)
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = 1)
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = .5)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = .3)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = .1)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = 10)
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = 10)
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = .1)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 200)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 200)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
dtgrid <- cbind(dtgrid, fitted(mdgrid))
colnames(dtgrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(dtgrid$X1, dtgrid$X2, col = dtgrid$Y, cex = .1)
#### AJUSTE: Regressão multinomial pelo pacote vgam
library(VGAM)
VGAM::vgam(Y~., data = train)
VGAM::vgam(Y~., data = train, family = "multinomial")
VGAM::vgam(Y~., data = train, family = "multinomial")
warnings()
VGAM::vglm(Y~., data = train, family = "multinomial")
VGAM::vglm(Y~., data = train, family = multinomial)
md <- VGAM::vglm(Y~., data = train, family = multinomial)
warnings()
fitted(md)
data(pneumo)
pneumo
md <- multinomial(Y~., data = train, family = multinomial)
md <- multinomial(Y~., data = train)
md <- vglm(Y~., data = train, family=multinomial(ynames = TRUE))
fitted(md)
?vglm
??multinom
md <- glm(Y~., data = train, family=multinomial)
md <- glm(Y~., data = train, family="multinomial")
md <- glm(Y~., data = train, family=multinomial())
md <- vglm(Y~., data = train, family=multinomial(ynames = F))
fitted(md)
md <- vglm(Y~., data = train, family=multinomial(ynames = T))
fitted(md)
predict(md, train)
?vglm
?nnet::multinom
example(birthwt)
example(bwt)
bwt
library(MASS)
bwt
example("birthwt")
example(birthwt)
bwt
unique(bwt$low)
install.packages("tidymodels")
install.packages("tidymodels")
md <- multinom(Y~., data = train, family=multinomial(ynames = T))
#### AJUSTE: Regressão multinomial pelo pacote vgam
md <- multinom(Y~., data = train)
predict(md, train)
summary(md)
multinomprd <- predict(md, train)
mcc_custom(train$Y, multinomprd)
#### AJUSTE: Regressão multinomial pelo pacote vgam
md <- multinom(Y~., data = train)
summary(md)
multinomprd <- predict(md, test)
mcc_custom(test$Y, multinomprd)
mcc_custom(test$Y, fitted(knn_model_final))
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 200)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 200)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
knngrid <- cbind(dtgrid, fitted(mdgrid))
colnames(knngrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .1)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .05)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
#### AJUSTE: Regressão multinomial pelo pacote vgam
mdmult <- multinom(Y~., data = train)
summary(mdmult)
multinomprd <- predict(mdmult, test)
mcc_custom(test$Y, multinomprd)
## Fronteiras de decisão
mdmultgrid <- cbind(dtgrid, predict(mdmult, dtgrid))
colnames(mdmultgrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .1)
?points
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, xy = .1)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cxy = .1)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cxy = .01)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 500)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 500)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
knngrid <- cbind(dtgrid, fitted(mdgrid))
colnames(knngrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .1)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .1)
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = alpha(mdmultgrid$Y, .1))
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = alpha(knngrid$Y, .1), cex = .01)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 100)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 100)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
knngrid <- cbind(dtgrid, fitted(mdgrid))
colnames(knngrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = alpha(knngrid$Y, .1), cex = .01)
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 150)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 150)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
knngrid <- cbind(dtgrid, fitted(mdgrid))
colnames(knngrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
#### AJUSTE: Regressão multinomial pelo pacote vgam
mdmult <- multinom(Y~., data = train)
summary(mdmult)
multinomprd <- predict(mdmult, test)
mcc_custom(test$Y, multinomprd)
## Fronteiras de decisão
mdmultgrid <- cbind(dtgrid, predict(mdmult, dtgrid))
colnames(mdmultgrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y)
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
?points
plot(train$X1, train$X2, col = train$Y, xlab = "X1", ylab = "X2")
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
plot(train$X1, train$X2, col = train$Y, xlab = "X1", ylab = "X2",
main = "Fronteiras de decisão: Regressão logística multinomial")
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
source("~/Documents/Teoria_Aprendizado_Estatistico/Atv2/main.R")
plot(train$X1, train$X2, col = train$Y, xlab = "X1", ylab = "X2",
main = "Fronteiras de decisão: Regressão multinomial via KNN")
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
library(nnet)
library(class)
library(kernlab)
library(kknn)
source("utils.R")
library(nnet)
library(class)
library(kernlab)
library(kknn)
source("utils.R")
link <- "https://raw.githubusercontent.com/andersonara/datasets/master/wall-robot-navigation.csv"
data <- read.csv(url(link), sep = ";")
data$Y <- factor(data$Y, ordered = FALSE)
#Separação - Treinamento e teste
size_tr <- round(.75 * nrow(data))
train_indx <- sample(1:nrow(data), size_tr, replace = FALSE)
train <- data[train_indx, ]
test <- data[-train_indx, ]
#Realização de validação holdout repetido
n_holdout <- 100
hold_prop <- .2
test_holdout_size <- round(hold_prop*nrow(train))
#K_grid
k_grid <- 1:20
mcc_vec <- numeric(length(k_grid))
for(i in 1:length(k_grid)){
mcc_avg <- numeric(n_holdout)
sapply(X = 1:n_holdout, function(x){
test_indx_holdout <- sample(1:nrow(train), test_holdout_size,
replace = FALSE)
holdout_train <- train[-test_indx_holdout, ]
holdout_test <- train[test_indx_holdout, -3]
holdout_test_labels <- train[test_indx_holdout, 3]
model <- kknn(Y~.,train = holdout_train, test = holdout_test,
kernel = "rectangular", k = k_grid[i])
mcc_avg[x] <<- mcc_custom(holdout_test_labels, fitted(model))
print(x)
})
mcc_vec[i] <- mean(mcc_avg)
}
selected_k <- k_grid[which.min(mcc_vec)]
knn_model_fit <- kknn(Y~.,train = train, test = train, k = selected_k,
kernel = "rectangular")
knn_model_final <- kknn(Y~.,train = train, test = test, k = selected_k,
kernel = "rectangular")
mcc_custom(test$Y, fitted(knn_model_final))
#Fronteira de decisão - dados de treinamento
x1_grid <- seq(min(train$X1), max(train$X1), length.out = 150)
x2_grid <- seq(min(train$X2), max(train$X2), length.out = 150)
dtgrid <- expand.grid(X1 = x1_grid, X2 = x2_grid)
mdgrid <- kknn(Y~., train = train, test = dtgrid, k = selected_k,
kernel = "rectangular")
knngrid <- cbind(dtgrid, fitted(mdgrid))
colnames(knngrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y, xlab = "X1", ylab = "X2",
main = "Fronteiras de decisão: Regressão multinomial via KNN")
points(knngrid$X1, knngrid$X2, col = knngrid$Y, cex = .01)
#### AJUSTE: Regressão multinomial pelo pacote vgam
mdmult <- multinom(Y~., data = train)
summary(mdmult)
multinomprd <- predict(mdmult, test)
mcc_custom(test$Y, multinomprd)
## Fronteiras de decisão
mdmultgrid <- cbind(dtgrid, predict(mdmult, dtgrid))
colnames(mdmultgrid)[3] <- "Y"
plot(train$X1, train$X2, col = train$Y, xlab = "X1", ylab = "X2",
main = "Fronteiras de decisão: Regressão logística multinomial")
points(mdmultgrid$X1, mdmultgrid$X2, col = mdmultgrid$Y, cex = .01)
